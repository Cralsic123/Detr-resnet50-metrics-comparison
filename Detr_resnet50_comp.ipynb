{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPCNYy51n0O1OYAc/Qb0wSf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "edd1e5e324314e54af70a9ec60f1909e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b4ec3c2e6c345c1a41961e66912db8b",
              "IPY_MODEL_c4fcd0d6b99a4db4b2f2daa37008dc22",
              "IPY_MODEL_74e3e0eb5574492daea35968976c54a6"
            ],
            "layout": "IPY_MODEL_61b8c62f3df9432d849a12258e1cf532"
          }
        },
        "7b4ec3c2e6c345c1a41961e66912db8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e986ec9bbf48461eb0657c62c6388b1c",
            "placeholder": "​",
            "style": "IPY_MODEL_920db4dc7cd044579576696df1a18155",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "c4fcd0d6b99a4db4b2f2daa37008dc22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8131781be63a460a9852acfd1cd3ec25",
            "max": 401,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3af9f52666c04144a873e72e18bef061",
            "value": 401
          }
        },
        "74e3e0eb5574492daea35968976c54a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_449656333dc741efb433b7ed51978d31",
            "placeholder": "​",
            "style": "IPY_MODEL_4c54c8d0d68648e28fbd855a23e8b470",
            "value": " 401/401 [00:00&lt;00:00, 21.8kB/s]"
          }
        },
        "61b8c62f3df9432d849a12258e1cf532": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e986ec9bbf48461eb0657c62c6388b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "920db4dc7cd044579576696df1a18155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8131781be63a460a9852acfd1cd3ec25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3af9f52666c04144a873e72e18bef061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "449656333dc741efb433b7ed51978d31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c54c8d0d68648e28fbd855a23e8b470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc240973ca5b4ffc973b5447f044b267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d0775f82ec24aa7adf17c4d4355a1ff",
              "IPY_MODEL_723d18f1aacc4643b9f2a90767027535",
              "IPY_MODEL_3cbf3d31df2a415085d77c4d0c69ec02"
            ],
            "layout": "IPY_MODEL_cf11abdbeccb4d248a84ead9ace2176b"
          }
        },
        "4d0775f82ec24aa7adf17c4d4355a1ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a839feb23cd64dc2b59475292303e105",
            "placeholder": "​",
            "style": "IPY_MODEL_a27e6b465bb14651a09eb50e6caf93be",
            "value": "config.json: 100%"
          }
        },
        "723d18f1aacc4643b9f2a90767027535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b86d3e7e8e1645ccb9d5a0c260fde798",
            "max": 6602,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8badb2608a24163be600ed34b8c77d5",
            "value": 6602
          }
        },
        "3cbf3d31df2a415085d77c4d0c69ec02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e03052c16d7a43a58a07422244915782",
            "placeholder": "​",
            "style": "IPY_MODEL_1d79de575ddf4974b36e86eb966151e9",
            "value": " 6.60k/6.60k [00:00&lt;00:00, 377kB/s]"
          }
        },
        "cf11abdbeccb4d248a84ead9ace2176b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a839feb23cd64dc2b59475292303e105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a27e6b465bb14651a09eb50e6caf93be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b86d3e7e8e1645ccb9d5a0c260fde798": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8badb2608a24163be600ed34b8c77d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e03052c16d7a43a58a07422244915782": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d79de575ddf4974b36e86eb966151e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1fad24934434521a9cc86e443dc0e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c62ed209e4d64efea05dbddb767c31a3",
              "IPY_MODEL_68cbbd96bf2a42ca9371a42184ccad16",
              "IPY_MODEL_2264d67b156d4320811de942b22c432b"
            ],
            "layout": "IPY_MODEL_3e64b6e859964216a5026abfaf74968e"
          }
        },
        "c62ed209e4d64efea05dbddb767c31a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fdcebf2fb954267845303aa8ea1e98a",
            "placeholder": "​",
            "style": "IPY_MODEL_1e8a7de3bc40464c83d1dba28bfc45b8",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "68cbbd96bf2a42ca9371a42184ccad16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28c2b1081fa74ec78d57b8d722fe5615",
            "max": 166708629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71c95f85b6bd42679749c5866d8631ba",
            "value": 166708629
          }
        },
        "2264d67b156d4320811de942b22c432b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8969479de20446318607d24d82c73457",
            "placeholder": "​",
            "style": "IPY_MODEL_0eaec10b6dcb482da9be520fd0dce6d4",
            "value": " 167M/167M [00:01&lt;00:00, 198MB/s]"
          }
        },
        "3e64b6e859964216a5026abfaf74968e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fdcebf2fb954267845303aa8ea1e98a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e8a7de3bc40464c83d1dba28bfc45b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28c2b1081fa74ec78d57b8d722fe5615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71c95f85b6bd42679749c5866d8631ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8969479de20446318607d24d82c73457": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eaec10b6dcb482da9be520fd0dce6d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cralsic123/Detr-resnet50-metrics-comparison/blob/main/Detr_resnet50_comp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65_vhILrehvD",
        "outputId": "189ab662-52d6-4872-a7f9-f7e2f0dae793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting grad-cam\n",
            "  Downloading grad-cam-1.5.0.tar.gz (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from grad-cam) (9.4.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.17.1+cu121)\n",
            "Collecting ttach (from grad-cam)\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.66.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from grad-cam) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7.1->grad-cam) (12.4.127)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.5.0-py3-none-any.whl size=38071 sha256=61440852d615fdaab4295d46af9908f97adbec488ee4891538eb2e7a079bc1fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/e5/3d/8548241d5cffe53ad1476c566a61ad9bf09cc61a9430f09726\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: ttach, grad-cam\n",
            "Successfully installed grad-cam-1.5.0 ttach-0.0.3\n",
            "Cloning into 'ai-hub-models'...\n",
            "remote: Enumerating objects: 5700, done.\u001b[K\n",
            "remote: Counting objects: 100% (1235/1235), done.\u001b[K\n",
            "remote: Compressing objects: 100% (445/445), done.\u001b[K\n",
            "remote: Total 5700 (delta 917), reused 937 (delta 789), pack-reused 4465\u001b[K\n",
            "Receiving objects: 100% (5700/5700), 254.95 MiB | 38.58 MiB/s, done.\n",
            "Resolving deltas: 100% (3721/3721), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install grad-cam\n",
        "!git clone https://github.com/quic/ai-hub-models.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "#from gradcam import GradCAM, show_cam_on_image\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "XnEAf9G_eyoL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DETR model\n"
      ],
      "metadata": {
        "id": "RVKFxrzKkwpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "ffL7O5r-mmQX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d navoneel/brain-mri-images-for-brain-tumor-detection\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51k0Gbvtmxpn",
        "outputId": "38461711-ac71-4d55-bdfa-eb7fd8553e1f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.6.14 / client 1.6.12)\n",
            "Dataset URL: https://www.kaggle.com/datasets/navoneel/brain-mri-images-for-brain-tumor-detection\n",
            "License(s): copyright-authors\n",
            "Downloading brain-mri-images-for-brain-tumor-detection.zip to /content\n",
            "  0% 0.00/15.1M [00:00<?, ?B/s]\n",
            "100% 15.1M/15.1M [00:00<00:00, 232MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/brain-mri-images-for-brain-tumor-detection.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./')\n"
      ],
      "metadata": {
        "id": "fIAGz2C4m2Ga"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n",
        "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")\n"
      ],
      "metadata": {
        "id": "zJRPh8KWg55X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "edd1e5e324314e54af70a9ec60f1909e",
            "7b4ec3c2e6c345c1a41961e66912db8b",
            "c4fcd0d6b99a4db4b2f2daa37008dc22",
            "74e3e0eb5574492daea35968976c54a6",
            "61b8c62f3df9432d849a12258e1cf532",
            "e986ec9bbf48461eb0657c62c6388b1c",
            "920db4dc7cd044579576696df1a18155",
            "8131781be63a460a9852acfd1cd3ec25",
            "3af9f52666c04144a873e72e18bef061",
            "449656333dc741efb433b7ed51978d31",
            "4c54c8d0d68648e28fbd855a23e8b470",
            "fc240973ca5b4ffc973b5447f044b267",
            "4d0775f82ec24aa7adf17c4d4355a1ff",
            "723d18f1aacc4643b9f2a90767027535",
            "3cbf3d31df2a415085d77c4d0c69ec02",
            "cf11abdbeccb4d248a84ead9ace2176b",
            "a839feb23cd64dc2b59475292303e105",
            "a27e6b465bb14651a09eb50e6caf93be",
            "b86d3e7e8e1645ccb9d5a0c260fde798",
            "f8badb2608a24163be600ed34b8c77d5",
            "e03052c16d7a43a58a07422244915782",
            "1d79de575ddf4974b36e86eb966151e9",
            "c1fad24934434521a9cc86e443dc0e99",
            "c62ed209e4d64efea05dbddb767c31a3",
            "68cbbd96bf2a42ca9371a42184ccad16",
            "2264d67b156d4320811de942b22c432b",
            "3e64b6e859964216a5026abfaf74968e",
            "8fdcebf2fb954267845303aa8ea1e98a",
            "1e8a7de3bc40464c83d1dba28bfc45b8",
            "28c2b1081fa74ec78d57b8d722fe5615",
            "71c95f85b6bd42679749c5866d8631ba",
            "8969479de20446318607d24d82c73457",
            "0eaec10b6dcb482da9be520fd0dce6d4"
          ]
        },
        "outputId": "b696a973-6152-4224-c3b6-840fa90bcefc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edd1e5e324314e54af70a9ec60f1909e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/6.60k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc240973ca5b4ffc973b5447f044b267"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/167M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1fad24934434521a9cc86e443dc0e99"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = '/content/brain_tumor_dataset/yes/Y1.jpg'\n",
        "image = Image.open(image_path)"
      ],
      "metadata": {
        "id": "u6ZewYa4g52t"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "sVFsoGBSg5fN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_sizes = torch.tensor([image.size[::-1]])\n",
        "results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n"
      ],
      "metadata": {
        "id": "hDM36EuznYEC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "    box = [round(i, 2) for i in box.tolist()]\n",
        "    print(\n",
        "            f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
        "            f\"{round(score.item(), 3)} at location {box}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "EJ1XDBz7nfQd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c2abd7-0379-44d7-f564-59de7b9e8314"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected cup with confidence 0.9 at location [18.09, 72.82, 90.13, 190.65]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.eval()"
      ],
      "metadata": {
        "id": "XX61Sp7foMH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "TO_f2IFfsDVc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/brain_tumor_dataset/yes'\n",
        "\n",
        "# List all image files in the folder\n",
        "image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('jpg', 'jpeg', 'JPG'))]\n"
      ],
      "metadata": {
        "id": "d-ElFdGTnhih"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for image_file in image_files:\n",
        "    image_path = os.path.join(folder_path, image_file)\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Preprocess the image\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    \n",
        "    # Forward pass through the model\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Post-process the results\n",
        "    target_sizes = torch.tensor([image.size[::-1]])\n",
        "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
        "\n",
        "    # Print the results for the current image\n",
        "    print(f\"Results for {image_file}:\")\n",
        "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
        "        box = [round(i, 2) for i in box.tolist()]\n",
        "        print(\n",
        "            f\"  Detected {model.config.id2label[label.item()]} with confidence \"\n",
        "            f\"{round(score.item(), 3)} at location {box}\"\n",
        "        )\n",
        "    print()"
      ],
      "metadata": {
        "id": "513L2B4rs6J3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grad Cam visualization"
      ],
      "metadata": {
        "id": "mzKA7wy8tp8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from torchvision import transforms\n",
        "#from datasets import load_dataset\n",
        "from pytorch_grad_cam import run_dff_on_image, GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from typing import List, Callable, Optional"
      ],
      "metadata": {
        "id": "Tg5Q3jkzoEJY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_tensor = transforms.ToTensor()(image)"
      ],
      "metadata": {
        "id": "O4STcFcbto8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class HuggingfaceToTensorModelWrapper(torch.nn.Module):\n",
        "#     def __init__(self, model):\n",
        "#         super(HuggingfaceToTensorModelWrapper, self).__init__()\n",
        "#         self.model = model\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.model(x).logits\n",
        "\n",
        "# \"\"\" Translate the category name to the category index.\n",
        "#     Some models aren't trained on Imagenet but on even larger datasets,\n",
        "#     so we can't just assume that 761 will always be remote-control.\n",
        "\n",
        "# \"\"\"\n",
        "# def category_name_to_index(model, category_name):\n",
        "#     name_to_index = dict((v, k) for k, v in model.config.id2label.items())\n",
        "#     return name_to_index[category_name]\n",
        "\n",
        "# \"\"\" Helper function to run GradCAM on an image and create a visualization.\n",
        "#     (note to myself: this is probably useful enough to move into the package)\n",
        "#     If several targets are passed in targets_for_gradcam,\n",
        "#     e.g different categories,\n",
        "#     a visualization for each of them will be created.\n",
        "\n",
        "# \"\"\"\n",
        "# def run_grad_cam_on_image(model: torch.nn.Module,\n",
        "#                           target_layer: torch.nn.Module,\n",
        "#                           targets_for_gradcam: List[Callable],\n",
        "#                           reshape_transform: Optional[Callable],\n",
        "#                           input_tensor: torch.nn.Module=img_tensor,\n",
        "#                           input_image: Image=image,\n",
        "#                           method: Callable=GradCAM):\n",
        "#     with method(model=HuggingfaceToTensorModelWrapper(model),\n",
        "#                  target_layers=[target_layer],\n",
        "#                  reshape_transform=reshape_transform) as cam:\n",
        "\n",
        "#         # Replicate the tensor for each of the categories we want to create Grad-CAM for:\n",
        "#         repeated_tensor = input_tensor[None, :].repeat(len(targets_for_gradcam), 1, 1, 1)\n",
        "\n",
        "#         batch_results = cam(input_tensor=repeated_tensor,\n",
        "#                             targets=targets_for_gradcam)\n",
        "#         results = []\n",
        "#         for grayscale_cam in batch_results:\n",
        "#             visualization = show_cam_on_image(np.float32(input_image)/255,\n",
        "#                                               grayscale_cam,\n",
        "#                                               use_rgb=True)\n",
        "#             # Make it weight less in the notebook:\n",
        "#             visualization = cv2.resize(visualization,\n",
        "#                                        (visualization.shape[1]//2, visualization.shape[0]//2))\n",
        "#             results.append(visualization)\n",
        "#         return np.hstack(results)\n",
        "\n",
        "\n",
        "# def print_top_categories(model, img_tensor, top_k=5):\n",
        "#     logits = model(img_tensor.unsqueeze(0)).logits\n",
        "#     indices = logits.cpu()[0, :].detach().numpy().argsort()[-top_k :][::-1]\n",
        "#     for i in indices:\n",
        "#         print(f\"Predicted class {i}: {model.config.id2label[i]}\")"
      ],
      "metadata": {
        "id": "lhAyESj5tiRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We will show GradCAM for the \"Egyptian Cat\" and the 'Remote Control\" categories:\n",
        "# I changed it to cup because the model is detecting the anomaly in the brain as a cup\n",
        "targets_for_gradcam = [ClassifierOutputTarget(category_name_to_index(model, \"cup\"))]"
      ],
      "metadata": {
        "id": "qk8eWl52oaiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl3EArf0vIus",
        "outputId": "c053eb2f-f536-4a2c-baf2-a628f3162a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DetrForObjectDetection(\n",
            "  (model): DetrModel(\n",
            "    (backbone): DetrConvModel(\n",
            "      (conv_encoder): DetrConvEncoder(\n",
            "        (model): ResNetBackbone(\n",
            "          (embedder): ResNetEmbeddings(\n",
            "            (embedder): ResNetConvLayer(\n",
            "              (convolution): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "              (normalization): DetrFrozenBatchNorm2d()\n",
            "              (activation): ReLU()\n",
            "            )\n",
            "            (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "          )\n",
            "          (encoder): ResNetEncoder(\n",
            "            (stages): ModuleList(\n",
            "              (0): ResNetStage(\n",
            "                (layers): Sequential(\n",
            "                  (0): ResNetBottleNeckLayer(\n",
            "                    (shortcut): ResNetShortCut(\n",
            "                      (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                      (normalization): DetrFrozenBatchNorm2d()\n",
            "                    )\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                  (1): ResNetBottleNeckLayer(\n",
            "                    (shortcut): Identity()\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                  (2): ResNetBottleNeckLayer(\n",
            "                    (shortcut): Identity()\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (1): ResNetStage(\n",
            "                (layers): Sequential(\n",
            "                  (0): ResNetBottleNeckLayer(\n",
            "                    (shortcut): ResNetShortCut(\n",
            "                      (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                      (normalization): DetrFrozenBatchNorm2d()\n",
            "                    )\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                  (1): ResNetBottleNeckLayer(\n",
            "                    (shortcut): Identity()\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                  (2): ResNetBottleNeckLayer(\n",
            "                    (shortcut): Identity()\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                  (3): ResNetBottleNeckLayer(\n",
            "                    (shortcut): Identity()\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (2): ResNetStage(\n",
            "                (layers): Sequential(\n",
            "                  (0): ResNetBottleNeckLayer(\n",
            "                    (shortcut): ResNetShortCut(\n",
            "                      (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                      (normalization): DetrFrozenBatchNorm2d()\n",
            "                    )\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                  (1): ResNetBottleNeckLayer(\n",
            "                    (shortcut): Identity()\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                  (2): ResNetBottleNeckLayer(\n",
            "                    (shortcut): Identity()\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                  (3): ResNetBottleNeckLayer(\n",
            "                    (shortcut): Identity()\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                  (4): ResNetBottleNeckLayer(\n",
            "                    (shortcut): Identity()\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                  (5): ResNetBottleNeckLayer(\n",
            "                    (shortcut): Identity()\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (3): ResNetStage(\n",
            "                (layers): Sequential(\n",
            "                  (0): ResNetBottleNeckLayer(\n",
            "                    (shortcut): ResNetShortCut(\n",
            "                      (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                      (normalization): DetrFrozenBatchNorm2d()\n",
            "                    )\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                  (1): ResNetBottleNeckLayer(\n",
            "                    (shortcut): Identity()\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                  (2): ResNetBottleNeckLayer(\n",
            "                    (shortcut): Identity()\n",
            "                    (layer): Sequential(\n",
            "                      (0): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (1): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): ReLU()\n",
            "                      )\n",
            "                      (2): ResNetConvLayer(\n",
            "                        (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                        (normalization): DetrFrozenBatchNorm2d()\n",
            "                        (activation): Identity()\n",
            "                      )\n",
            "                    )\n",
            "                    (activation): ReLU()\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (position_embedding): DetrSinePositionEmbedding()\n",
            "    )\n",
            "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (query_position_embeddings): Embedding(100, 256)\n",
            "    (encoder): DetrEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0-5): 6 x DetrEncoderLayer(\n",
            "          (self_attn): DetrAttention(\n",
            "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (activation_fn): ReLU()\n",
            "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (decoder): DetrDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0-5): 6 x DetrDecoderLayer(\n",
            "          (self_attn): DetrAttention(\n",
            "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (activation_fn): ReLU()\n",
            "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): DetrAttention(\n",
            "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (class_labels_classifier): Linear(in_features=256, out_features=92, bias=True)\n",
            "  (bbox_predictor): DetrMLPPredictionHead(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# target_layer = model.model.decoder.layernorm\n",
        "# classifier = model.class_labels_classifier"
      ],
      "metadata": {
        "id": "aLcBQw5WwxMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The last layer in the Resnet Encoder:\n",
        "#target_layer = model.encoder.stages[-1].layers[-1]\n",
        "#target_layer = model.ResNetEncoder.stages[-1].layers[-1]\n",
        "#target_layer = model.model.backbone.conv_encoder.model.encoder.stages[-1].layers[-1]\n",
        "model.model.decoder.layernorm\n",
        "\n",
        "display(Image.fromarray(run_dff_on_image(model=model,\n",
        "                          target_layer=target_layer,\n",
        "                          classifier=classifier,\n",
        "                          img_pil=image,\n",
        "                          img_tensor=img_tensor,\n",
        "                          reshape_transform=None,\n",
        "                          n_components=4,\n",
        "                          top_k=2)))\n",
        "display(Image.fromarray(run_grad_cam_on_image(model=model,\n",
        "                      target_layer=target_layer,\n",
        "                      targets_for_gradcam=targets_for_gradcam,\n",
        "                      reshape_transform=None)))\n",
        "print_top_categories(model, img_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "pcJlD9YDtzaP",
        "outputId": "c6dcc2b3-71e3-4360-ac36-e7e0e6343fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (4x2048 and 256x92)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-cbfa0afe8025>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m display(Image.fromarray(run_dff_on_image(model=model,\n\u001b[0m\u001b[1;32m      8\u001b[0m                           \u001b[0mtarget_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                           \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_grad_cam/feature_factorization/deep_feature_factorization.py\u001b[0m in \u001b[0;36mrun_dff_on_image\u001b[0;34m(model, target_layer, classifier, img_pil, img_tensor, reshape_transform, n_components, top_k)\u001b[0m\n\u001b[1;32m    113\u001b[0m                                    computation_on_concepts=classifier)\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     concepts, batch_explanations, concept_outputs = dff(\n\u001b[0m\u001b[1;32m    116\u001b[0m         img_tensor[None, :], n_components)\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_grad_cam/feature_factorization/deep_feature_factorization.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_tensor, n_components)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 concept_tensors = torch.from_numpy(\n\u001b[1;32m     79\u001b[0m                     np.float32(concepts).transpose((1, 0)))\n\u001b[0;32m---> 80\u001b[0;31m                 concept_outputs = self.computation_on_concepts(\n\u001b[0m\u001b[1;32m     81\u001b[0m                     concept_tensors).cpu().numpy()\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconcepts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_explanations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcept_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x2048 and 256x92)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x2048 and 256x92) indicates that there's a mismatch between the shapes of the matrices being multiplied, likely due to the wrong layer being used as target_layer. The layernorm layer has an output of shape (batch_size, sequence_length, 256), which is not directly compatible with the classifier expecting an input of shape (batch_size, 256).\n",
        "\n",
        "To solve this, we should use the correct layer's output, which matches the input expected by the classifier. The target_layer should be the last layer in the decoder before the classifier. In the DETR model, this is typically the output of the decoder."
      ],
      "metadata": {
        "id": "ne4bb-jWxYlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying DFF (Dimensionality Filter Framework) output\n",
        "dff_image = run_dff_on_image(\n",
        "    model=model,\n",
        "    target_layer=target_layer,\n",
        "    classifier=classifier,\n",
        "    img_pil=image,\n",
        "    img_tensor=img_tensor,\n",
        "    reshape_transform=None,\n",
        "    n_components=4,\n",
        "    top_k=2\n",
        ")\n",
        "display(Image.fromarray(dff_image))\n",
        "\n",
        "# Displaying Grad-CAM output\n",
        "grad_cam_image = run_grad_cam_on_image(\n",
        "    model=model,\n",
        "    target_layer=target_layer,\n",
        "    targets_for_gradcam=targets_for_gradcam,\n",
        "    reshape_transform=None\n",
        ")\n",
        "display(Image.fromarray(grad_cam_image))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "xWUbqGYbt7Q3",
        "outputId": "e6e27d72-8bf5-4eba-ff41-138e1f037377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (4x2048 and 256x92)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-3d5cf71f940c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Displaying DFF (Dimensionality Filter Framework) output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m dff_image = run_dff_on_image(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtarget_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_grad_cam/feature_factorization/deep_feature_factorization.py\u001b[0m in \u001b[0;36mrun_dff_on_image\u001b[0;34m(model, target_layer, classifier, img_pil, img_tensor, reshape_transform, n_components, top_k)\u001b[0m\n\u001b[1;32m    113\u001b[0m                                    computation_on_concepts=classifier)\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     concepts, batch_explanations, concept_outputs = dff(\n\u001b[0m\u001b[1;32m    116\u001b[0m         img_tensor[None, :], n_components)\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_grad_cam/feature_factorization/deep_feature_factorization.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_tensor, n_components)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 concept_tensors = torch.from_numpy(\n\u001b[1;32m     79\u001b[0m                     np.float32(concepts).transpose((1, 0)))\n\u001b[0;32m---> 80\u001b[0;31m                 concept_outputs = self.computation_on_concepts(\n\u001b[0m\u001b[1;32m     81\u001b[0m                     concept_tensors).cpu().numpy()\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconcepts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_explanations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcept_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x2048 and 256x92)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Let's define a helper function to find the correct target layer\n",
        "def get_target_layer_and_output(model, img_tensor):\n",
        "    # Forward pass to find the correct layer output\n",
        "    outputs = model.model.backbone(img_tensor)\n",
        "    target_layer_output = outputs  # Adjust this according to actual layer outputs\n",
        "    return model.model.backbone, target_layer_output\n",
        "\n",
        "# Get the target layer and output\n",
        "target_layer, target_layer_output = get_target_layer_and_output(model, img_tensor)\n",
        "\n",
        "# Displaying DFF (Dimensionality Filter Framework) output\n",
        "try:\n",
        "    dff_image = run_dff_on_image(\n",
        "        model=model,\n",
        "        target_layer=target_layer,\n",
        "        classifier=classifier,\n",
        "        img_pil=image,\n",
        "        img_tensor=img_tensor,\n",
        "        reshape_transform=None,\n",
        "        n_components=4,\n",
        "        top_k=2\n",
        "    )\n",
        "    display(Image.fromarray(dff_image))\n",
        "except RuntimeError as e:\n",
        "    print(f\"RuntimeError during DFF: {e}\")\n",
        "\n",
        "# Displaying Grad-CAM output\n",
        "try:\n",
        "    grad_cam_image = run_grad_cam_on_image(\n",
        "        model=model,\n",
        "        target_layer=target_layer,\n",
        "        targets_for_gradcam=targets_for_gradcam,\n",
        "        reshape_transform=None\n",
        "    )\n",
        "    display(Image.fromarray(grad_cam_image))\n",
        "except RuntimeError as e:\n",
        "    print(f\"RuntimeError during Grad-CAM: {e}\")\n",
        "\n",
        "# Printing the top categories\n",
        "try:\n",
        "    print_top_categories(model, img_tensor)\n",
        "except RuntimeError as e:\n",
        "    print(f\"RuntimeError during category printing: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "94aIQRKuxC0r",
        "outputId": "f1ce9ca0-a308-45d5-d843-b0ea0ef55a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "DetrConvModel.forward() missing 1 required positional argument: 'pixel_mask'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-09d2659d5098>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Get the target layer and output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtarget_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_target_layer_and_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Displaying DFF (Dimensionality Filter Framework) output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-09d2659d5098>\u001b[0m in \u001b[0;36mget_target_layer_and_output\u001b[0;34m(model, img_tensor)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_target_layer_and_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Forward pass to find the correct layer output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtarget_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m  \u001b[0;31m# Adjust this according to actual layer outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_layer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: DetrConvModel.forward() missing 1 required positional argument: 'pixel_mask'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YweGApHBxihu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}